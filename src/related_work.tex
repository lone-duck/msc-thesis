
\chapter{Related Work}
\section{Video Inpainting} 
Recent advances in video inpainting have largely been driven by methods which fill in masked regions by borrowing content from the unmasked regions of other frames. These methods typically use optical flow estimates \citep{temporally, deepvideoinpainting, dfvi, flowedgeguided}, self-attention \citep{learningjoint, fuseformer, onionpeel, copypaste} or a combination of both \citep{propainter, fgt, endtoend} to determine how to propagate pixel values or learned features across frames. Such methods often produce visually compelling results, particularly on tasks where the mask-occluded region is visible in nearby frames such as foreground object removal. They struggle, however, in cases where this does not hold, \eg in the presence of heavy camera motion, large masks, or tasks where semantic understanding of the video content is required to produce a convincing result.

More recent work has utilized diffusion models for video inpainting. \citet{lookmanohands} uses a latent diffusion model \citep{stablediffusion, vahdat2021score} to remove the agent's view of itself from egocentric videos for applications in robotics. Notably, this is framed as an image inpainting task, where the goal is to remove the agent (with a mask provided by a segmentation model) from a single video frame conditioned on $h$ previous frames. Consequently, the results lack temporal consistency when viewed as videos, and the model is evaluated using image inpainting metrics only. \citet{avid} proposes a method for the related task of text-conditional video inpainting, which produces impressive results but requires user intervention. Most similar to this work, \citet{fgdvi} proposes a method for video inpainting that combines a video diffusion model with optical flow guidance. 


\section{Image Inpainting with Diffusion Models}
This work takes inspiration from the recent success of diffusion models for image inpainting. These methods can be split into two groups: those that inpaint using an unconditional diffusion model by making heuristic adjustments to the sampling procedure \citep{repaint, copaint}, and those that explicitly train a conditional diffusion model which, if sufficiently expressive and trained to optimality, enables exact sampling from the conditional distribution \citep{palette,zhang2023adding}. We follow the latter approach in this work.









